:toc:
:toc-placement!:
toc::[]

* Installs Oracle RAC, RAC One Node and normal single instances.
* Start with one or more clean machine(s), end up with a fully
configured RAC Cluster.

= Getting started

== Pre-requisites

* Ansible >= 2.9 +
  2.12 is recommended
* Oracle Linux (or any RHEL-based Linux System) >= 7 +
  6 is end of life and not tested anymore.
* Oracle Database/Grid Infrastructure 21.3.0.0, 19.3.0.0 +
  was supported and not tested anymore: 12.2.0.1, 12.1.0.2,
12.1.0.1, 11.2.0.4, 11.2.0.3
* For example configurations, look in:

## Example installation

----
SI/FS:   inventory/dbfs/hosts.yml  - vagrant config: vagrant/dbfs
----

By default, installs a single instance 19.3.0.0 database on a filesystem.

### Single Instance with Filesystem in Vagrantbox

#### Ansible-Controller

Requirements::
Ansible 2.9-2.12

IMPORTANT: The `git clone` is only needed, when the inventory from ansible-oracle is used. +
Please use a custom Repository with `req√∫irements.yml` for your environment. +
An example is shown in https://github.com/opitzconsulting/ansible-oracle-inventory

.clone Repository and import ansible-oracle collection
----
git clone https://github.com/oravirt/ansible-oracle
ansible-galaxy collection install -r ansible-oracle/requirements.yml
----

#### Vagrantbox for Database Server

Requirements::
4 GB RAM: +
20 GB Diskspace:

The Inventory from dbfs expects a 2nd disk at `/dev/sdb`.
See `host_fs_layout` as example.

The Vagrantfile is in `vagrant/dbfs` of `ansible-oracle`.

IMPORTANT: The `VAGRANT_EXPERIMENTAL=disks` is very important.
It adds the 2nd disk to the VM during the startup.

.Start Vagrantbox
----
cd vagrant/dbfs
VAGRANT_EXPERIMENTAL=disks vagrant up
----

IMPORTANT: Copy `LINUX.X64_190000_db_home.zip` into `/tmp` on Vagrantbox


#### Start Playbook

IMPORTANT: The used Vagrantbox does not configure the `/etc/hosts` for Oracle correctly.
The `playbook/os_vagrant.yml` fixes that.

.execute playbook
----
ansible-playbook -e hostgroup=all -i inventory/dbfs/hosts.yml playbooks/os_vagrant.yml playbooks/single-instance-fs.yml 
----

= Roles

== common

This will configure stuff common to all machines

* Install some generic packages
* Configure ntp

== orahost

This will configure the host specific Oracle stuff:

* Add a user & group
* Create directory structures
* Handle filesystem storage (partition devices, creates vg/lv and a
filesystem (ext4, xfs, btrfs) etc). If you want to create your database
on a filesystem (instead of ASM) this is where you define the layout.
* Install required packages
* Change kernel paramemeters
* Set up pam.d/limits config
* Configures Hugepages (as a percentage of total RAM)
* Disables transparent hugepages
* Disables NUMA (if needed)
* Configures the interconnect network (if needed)
* Configures Oracle ASMLib

== orahost-ssh

Configures passwordless ssh between clusternodes if setting up RAC
(`configure_cluster=True`)

* Uses existing ssh-keys

== orahost-storage

This role configures storage that shoud be used by ASM.

* Partitions devices (using parted)
* Create ASMlib labels or sets up udev-rules for device name persistence

== oraswgi-install

This role will install and configure Oracle Grid Infrastructure (RAC/SI)

* Adds a .profile_grid to the oracle user
* Sets up directory structures
* Copies the install-files to the servers, or installs from a remote
location (e.g nfs share)
* Install Oracle Grid Infrastructure

== oraasm-manage-diskgroups

This role will statefully manage the lifecycle of an ASM diskgroup

* Uses the *oracle_asmdg* module
* Create/delete diskgroup.
* Add/remove disks
* Manage attributes for the DG

== oraswdb-install

This role will install the oracle database server(s). It is possible to
run more than 1 database from each home. It performs both Single
Instance/RAC installations.

* Creates a .profile with the correct environment
* Creates directory structures
* Installs the database-server(s)

== oradb-manage-db

This role statefully manages the lifecycle of a database

* Uses the *oracle_db* module
* Creates/deletes: `state: present/absent`
* Maintains archivelog/force_logging True/False

== oraswgi-manage-patches

Manage patches in a GI environment

* Uses the *oracle_opatch* module
* Manages opatchauto type of patches as well as 'normal' one-offs

== oraswdb-manage-patches

Statefully manage patches in a DB environment

* Uses the *oracle_opatch* module
* Manages opatchauto type of patches as well as 'normal' one-offs

== cxoracle

Installs cx_Oracle in preparation for using
https://github.com/oravirt/ansible-oracle-modules[ansible-oracle-modules]

== orahost-cron

Configures cron schedules if needed

== orahost-logrotate

== oradb-manage-<*>

Statefully manages various aspects of the DB. They all use modules from
https://github.com/oravirt/ansible-oracle-modules[ansible-oracle-modules]

* *oradb-manage-pdb*
* *oradb-manage-tablespace*
* *oradb-manage-parameters*
* *oradb-manage-roles*
* *oradb-manage-users*
* *oradb-manage-grants*
* *oradb-manage-redo*
* *oradb-manage-services*

= Note

These are the Oracle binaries that are pre-configured to be used.
They
have to be manually downloaded and made available (either locally, from
a web endpoint or through a nfs-share)


.For 18.3.0.0:
----
LINUX.X64_180000_db_home.zip
LINUX.X64_180000_grid_home.zip
----

.For 12.2.0.1:
----
linuxx64_12201_database.zip
linuxx64_12201_grid_home.zip
----

.For 12.1.0.2
----
linuxamd64_12102_database_1of2.zip
linuxamd64_12102_database_2of2.zip
linuxamd64_12102_grid_1of2.zip
linuxamd64_12102_grid_2of2.zip
----

.For 12.1.0.1:
----
linuxamd64_12c_database_1of2.zip
linuxamd64_12c_database_2of2.zip
linuxamd64_12c_grid_1of2.zip
linuxamd64_12c_grid_2of2.zip
----

.For 11.2.0.4:
----
p13390677_112040_Linux-x86-64_1of7.zip
p13390677_112040_Linux-x86-64_2of7.zip
p13390677_112040_Linux-x86-64_3of7.zip
----

.For 11.2.0.3:
----
p10404530_112030_Linux-x86-64_1of7.zip
p10404530_112030_Linux-x86-64_2of7.zip
p10404530_112030_Linux-x86-64_3of7.zip
----

= Pull-Requests


== Important Information

The ansible-oracle project introduced `antsibull-changelog` for managing the `CHANGELOG.rst` based on fragments in `changelogs/gragments`.

The ID should point to the PR and the filename describe the PR in short form.
The fragments are part of the PR.
If multiple PRs are open, the upper rule makes sure that no duplicate files are created during merge.

IMPORTANT: Each Pull-Requests needs a fragment from Release 3.0.0 onwards!

== Working with antsibull-changelog

Changelogs for Collections: https://github.com/ansible-community/antsibull-changelog/blob/main/docs/changelogs.rst#releasing-a-new-version-of-a-collection

== Creating new releases

`antsibull-changelog release` reads `galaxy.yml` to get the release version automatically.
The execution is aborted, when a release with the version is existing in `CHANGELOG.rst`.

NOTE: The whole release process should be donw with a dedicated Pull-Request.
